# .NET Zero-Data Copy API (Snowflake & Databricks)
Full-stack developers intro to eliminating data duplication, reducing storage costs, and ensuring access of the most up-to-date information.  Also referred to as Shallow Copy/Clone.

## Snowflake's Zero-Copy Cloning 
Steps to setting up Snowflake.
Run the following SQL commands in a worksheet:
```
-- 1. Create a Database
CREATE DATABASE BC_USERS;

-- 2. Create a Schema within the new Database
CREATE SCHEMA BC_USERS.PUBLIC;

-- 3. Create a Table called USER_LOCATION
-- You should first ensure you are using the correct database and schema, or qualify the name
USE SCHEMA BC_USERS.PUBLIC;
create or replace TABLE BC_USERS.PUBLIC.USER_LOCATION (
	USER_LOCATION_ID NUMBER(38,0) NOT NULL autoincrement start 1 increment 1 noorder,
	FIRST_NAME VARCHAR(255) NOT NULL,
	LAST_NAME VARCHAR(255) NOT NULL,
	EMAIL VARCHAR(255) NOT NULL,
	ZIP VARCHAR(10) NOT NULL,
	unique (EMAIL),
	primary key (USER_LOCATION_ID)
);

--------------------------------------------------------------------------------

-- 4. Create a Role called MY_APPLICATION_ROLE
CREATE ROLE MY_APPLICATION_ROLE;

-- 5. Grant USAGE on the Warehouse to the Role (Correct Grant)
-- NOTE: I'm assuming SNOWFLAKE_LEARNING_WH already exists.
GRANT USAGE ON WAREHOUSE SNOWFLAKE_LEARNING_WH TO ROLE MY_APPLICATION_ROLE;

-- 6. Grant SELECT/INSERT/UPDATE/DELETE on the Table to the Role (Correct Grant)
-- USAGE is for databases/schemas/warehouses, not tables.
-- You usually grant DML (Data Manipulation Language) privileges for tables.
GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE BC_USERS.PUBLIC.USER_LOCATION TO ROLE MY_APPLICATION_ROLE;

CREATE USER DOTNET_API_USER
PASSWORD = '<<SOME_PASSWORD_VALUE>>'
DEFAULT_ROLE = MY_APPLICATION_ROLE;

GRANT ROLE MY_APPLICATION_ROLE TO USER DOTNET_API_USER;
```
Import that found_users.csv into the USER_LOCATION table

### Import ZipCode Metadata into Snowflake
Link to DeepSync U.S. ZIP Code Metadata via Snowflake's Marketplace:
https://app.snowflake.com/marketplace/listing/GZT1ZJ0SBC/deep-sync-u-s-zip-code-metadata

Upon importing the marketplace data, need to grant privileges to your database role that is associated to your database user, something like the following command:

`GRANT IMPORTED PRIVILEGES ON DATABASE U_S__ZIP_CODE_METADATA TO ROLE MY_APPLICATION_ROLE;`

### .NET
Update your app.settings.json with your credentials:
```
  "DataPlatform": {
		"Vendor": "Snowflake"
	},
	"ConnectionStrings": {
		"SnowflakeConnection": "ACCOUNT=<<YOUR_ACCOUNT>>;USER=<<YOUR_USER_ID>>;PASSWORD=<<YOUR_PASSWORD>>;WAREHOUSE=<<YOUR_WAREHOUSE>>;ROLE=<<YOUR_ROLE>>;"
	}
```
## Databricks' Shallow Clone
Steps to setting up Databricks.
Run the following SQL commands in a notebook:
```
-- Step 1: Create the Catalog
CREATE CATALOG IF NOT EXISTS BC_USERS;

-- Step 2: Set the Working Catalog and Create the Schema
USE CATALOG BC_USERS;
CREATE SCHEMA IF NOT EXISTS PUBLIC;

-- Step 3: Set the Working Schema
USE SCHEMA PUBLIC;

-- Step 4: Create the Table with the Auto-Increment Column
CREATE TABLE USER_LOCATION (
    USER_LOCATION_ID BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
    FIRST_NAME STRING,
    LAST_NAME STRING,
    EMAIL STRING,
    ZIP STRING
)
USING DELTA
COMMENT 'User location data with an auto-generated primary key';

-- Optional: Verify the table was created
DESCRIBE EXTENDED USER_LOCATION;
```
Upload the found_users.csv to a volume in your BC_USERS.PUBLIC schema
Import that CSV with the following command:
```
-- Insert data from the CSV file into the target table
INSERT INTO USER_LOCATION (
    FIRST_NAME,
    LAST_NAME,
    EMAIL,
    ZIP
)
SELECT
    t.FIRST_NAME, -- Alias for the first column in the CSV
    t.LAST_NAME,  -- Alias for the second column in the CSV
    t.EMAIL,     -- Alias for the third column in the CSV
    CAST(ZIP AS INT)    -- Alias for the fourth column in the CSV
FROM
    -- Use the READ_FILES function to read the data
    READ_FILES(
        '/Volumes/bc_users/public/test/databricks.csv',
        format => 'csv',
        header => true          -- Assumes the first row of your CSV is a header
    ) AS t;
```
Create an SQL function for haversine (calculating distance between two points on a sphere) for calculating proximity to a zipcode
```
CREATE OR REPLACE FUNCTION bc_users.public.HAVERSINE(
    lat1 DOUBLE, 
    lon1 DOUBLE, 
    lat2 DOUBLE, 
    lon2 DOUBLE
)
RETURNS DOUBLE
LANGUAGE PYTHON
AS $$
import math

def haversine_distance(lat1, lon1, lat2, lon2):
    # Earth's radius in kilometers
    R = 6371.0

    lat1_rad = math.radians(lat1)
    lon1_rad = math.radians(lon1)
    lat2_rad = math.radians(lat2)
    lon2_rad = math.radians(lon2)

    dlon = lon2_rad - lon1_rad
    dlat = lat2_rad - lat1_rad

    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    distance = R * c
    return distance

return haversine_distance(lat1, lon1, lat2, lon2)
$$;
```
### Import ZipCode Metadata into Databricks
Import DeepSync's US Zip Code Metadata from the Databricks marketplace by clicking "Get instant access"
Upon importing, go to the "Catalog" view and simplify the name of the database from "deep_sync_us_zip_code_metadata_populations_geo_centroid_lat_lng_city_names_state_dma_demographics" to "U_S__ZIP_CODE_METADATA"

### .NET
Update your app.settings.json with your credentials:
```
  "DataPlatform": {
		"Vendor": "Databricks"
	},
	"ConnectionStrings": {
		"DatabricksConnection": "Server=<<YOUR_SERVER>>;HTTPPath=<<YOUR_HTTP_PATH>>;Token=<<YOUR_PERSONAL_ACCESS_TOKEN>>;Catalog=<<YOUR_CATALOG>>;Database=<<YOUR_SCHEMA>>;"
	}
```
